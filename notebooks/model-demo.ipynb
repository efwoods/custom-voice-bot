{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09ffe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing LoRA adapter from ./qlora_adapter/checkpoint-18/\n",
      "trainable params: 0 || all params: 3,215,043,584 || trainable%: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46/46 [00:00<00:00, 2004.82 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 2432.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Base model and tokenizer (QLoRA-ready model)\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "ADAPTER = \"meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\"\n",
    "PEFT_DIR = \"./qlora_adapter/checkpoint-18/\"\n",
    "VECTORSTORE_DIR = \"./chroma_db\"\n",
    "\n",
    "# Sentence embedding model for vector store and evaluation\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Setup Chroma client and collection\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "client = PersistentClient(path=VECTORSTORE_DIR)\n",
    "collection = client.get_or_create_collection(name=\"style_memory\")\n",
    "\n",
    "\n",
    "LOAD_MODEL = True\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer_for_qlora():\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # 4-bit quantization config for QLoRA\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    # Load model in 4-bit for QLoRA\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # If adapter weights already exist, load them\n",
    "    if LOAD_MODEL:\n",
    "        print(f\"Loading existing LoRA adapter from {PEFT_DIR}\")\n",
    "        # Load the fine-tuned QLoRA adapter weights with PeftModel.from_pretrained\n",
    "        model = PeftModel.from_pretrained(model, PEFT_DIR)\n",
    "    else:\n",
    "        print(\"Preparing Model for Training\")\n",
    "        # Prepare for k-bit training (adds norm casting, disables gradients on frozen parts, etc.)\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "        # LoRA adapter configuration (adapt r, alpha, target_modules as needed)\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "\n",
    "        # Wrap the model with PEFT\n",
    "        model = get_peft_model(model, lora_config)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer_for_qlora()\n",
    "\n",
    "data_dir = \"../data/prompt_response/\"\n",
    "data_file_l = glob(data_dir + \"*.json\")\n",
    "data_file_l[0]\n",
    "with open(data_file_l[0], 'rb') as f:\n",
    "    data = json.load(f)\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def group_texts(examples):\n",
    "    # labels are input_ids with padding tokens masked as -100 to ignore in loss\n",
    "    labels = []\n",
    "    for input_ids in examples[\"input_ids\"]:\n",
    "        label = input_ids.copy()\n",
    "        # Mask padding tokens\n",
    "        label = [-100 if token == tokenizer.pad_token_id else token for token in label]\n",
    "        labels.append(label)\n",
    "    examples[\"labels\"] = labels\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(group_texts, batched=True)\n",
    "\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split['train']\n",
    "eval_dataset = split['test']\n",
    "\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./qlora_adapter\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     eval_steps=100,\n",
    "#     save_steps=100,\n",
    "#     logging_steps=10,\n",
    "#     learning_rate=2e-4,\n",
    "#     num_train_epochs=3,\n",
    "#     save_total_limit=2,\n",
    "#     bf16=True if torch.cuda.is_bf16_supported() else False,\n",
    "#     gradient_checkpointing=False,\n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "\n",
    "def generate_with_context(user_input: str, top_k: int = 10, max_new_tokens: int = 50) -> str:\n",
    "    # Embed query and retrieve from vector DB\n",
    "    query_vec = embedder.encode(user_input).tolist()\n",
    "    results = collection.query(query_embeddings=[query_vec], n_results=top_k)\n",
    "    print(f\"results: {results}\")\n",
    "\n",
    "    docs = results.get(\"documents\", [[]])[0]\n",
    "    if not docs:\n",
    "        context = \"No relevant context found.\"\n",
    "    else:\n",
    "        # Optionally truncate context length for model input token limits\n",
    "        # Here we join and limit length (e.g., first 1000 chars)\n",
    "        context = \"\\n\".join(docs)\n",
    "        context = context[:1000]\n",
    "\n",
    "    # Improved prompt with explicit instruction and clear delimiters\n",
    "    prompt = f\"\"\"You are the person in the contextual statements. Use the context to answer the question briefly and only once.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Q: {user_input}\n",
    "A:\"\"\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
    "\n",
    "    # Generate answer\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Set True to enable sampling\n",
    "            # temperature=0.7,  # Uncomment if do_sample=True\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract text after last \"A:\" in case prompt or output has multiple\n",
    "    answer = decoded.split(\"A:\")[-1].strip()\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aa41aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results: {'ids': [['doc_28', 'doc_32', 'doc_4', 'doc_18', 'doc_17', 'doc_45', 'doc_34', 'doc_19', 'doc_15', 'doc_14']], 'embeddings': None, 'documents': [['I love YOU!!!', 'I love you too sweetie.... Thanks for your help!', 'Love you both ..... I am fine', 'I love you Evan Woods!', 'Ditto!!!!', 'You are a good person', 'Thanks for bringing me to work tonight! Sweet dreams handsome!!! Love u bunches!!!!', 'Good night', \"I'm bringing breakfast home!\", \"If it's going to b this cold, I want snow!!!!\"]], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[None, None, None, None, None, None, None, None, None, None]], 'distances': [[0.6161439418792725, 1.017336368560791, 1.1762645244598389, 1.238104224205017, 1.344528317451477, 1.3923487663269043, 1.42818284034729, 1.4650744199752808, 1.4800461530685425, 1.5293219089508057]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I love you, sweetie!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_context(user_input=input())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
