{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac616a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Base model and tokenizer (QLoRA-ready model)\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "ADAPTER = \"meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\"\n",
    "\n",
    "PEFT_DIR = \"./qlora_adapter\"  # Directory to save/load adapter weights\n",
    "VECTORSTORE_DIR = \"./chroma_db\"\n",
    "\n",
    "# Sentence embedding model for vector store and evaluation\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Setup Chroma client and collection\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "client = PersistentClient(path=VECTORSTORE_DIR)\n",
    "collection = client.get_or_create_collection(name=\"style_memory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ffe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "PEFT_DIR = \"./qlora_adapter/checkpoint-18/\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LOAD_MODEL = False\n",
    "\n",
    "def load_model_and_tokenizer_for_qlora():\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # 4-bit quantization config for QLoRA\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    # Load model in 4-bit for QLoRA\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # If adapter weights already exist, load them\n",
    "    if LOAD_MODEL:\n",
    "        print(f\"Loading existing LoRA adapter from {PEFT_DIR}\")\n",
    "        # Load the fine-tuned QLoRA adapter weights with PeftModel.from_pretrained\n",
    "        model = PeftModel.from_pretrained(model, PEFT_DIR)\n",
    "    else:\n",
    "        print(\"Preparing Model for Training\")\n",
    "        # Prepare for k-bit training (adds norm casting, disables gradients on frozen parts, etc.)\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "        # LoRA adapter configuration (adapt r, alpha, target_modules as needed)\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "\n",
    "        # Wrap the model with PEFT\n",
    "        model = get_peft_model(model, lora_config)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer_for_qlora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34885728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "data_dir = \"../data/prompt_response/\"\n",
    "data_file_l = glob(data_dir + \"*.json\")\n",
    "data_file_l[0]\n",
    "with open(data_file_l[0], 'rb') as f:\n",
    "    data = json.load(f)\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def group_texts(examples):\n",
    "    # labels are input_ids with padding tokens masked as -100 to ignore in loss\n",
    "    labels = []\n",
    "    for input_ids in examples[\"input_ids\"]:\n",
    "        label = input_ids.copy()\n",
    "        # Mask padding tokens\n",
    "        label = [-100 if token == tokenizer.pad_token_id else token for token in label]\n",
    "        labels.append(label)\n",
    "    examples[\"labels\"] = labels\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(group_texts, batched=True)\n",
    "\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split['train']\n",
    "eval_dataset = split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_adapter\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    bf16=True if torch.cuda.is_bf16_supported() else False,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
