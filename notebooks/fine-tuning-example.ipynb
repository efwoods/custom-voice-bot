{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849869a5",
   "metadata": {},
   "source": [
    "# Fine-tuning LLMs on personal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c01abc29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m data_dir = \u001b[33m\"\u001b[39m\u001b[33m../data/prompt_response/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m data_file_l = \u001b[43mglob\u001b[49m(data_dir + \u001b[33m\"\u001b[39m\u001b[33m*.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m data_file_l[\u001b[32m0\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_file_l[\u001b[32m0\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/prompt_response/\"\n",
    "data_file_l = glob(data_dir + \"*.json\")\n",
    "data_file_l[0]\n",
    "with open(data_file_l[0], 'rb') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b11e05",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## [Tutorial Link ](https://learn.deeplearning.ai/courses/finetuning-large-language-models/lesson/vl60i/training-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a450e",
   "metadata": {},
   "source": [
    "### Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f56656",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5442bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(text)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84687411",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize multiple texts at once\n",
    "encoded_texts = tokenizer(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a55f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoded several texts: \", encoded_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71373fc6",
   "metadata": {},
   "source": [
    "### Padding and truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f063858",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encoded_texts_longest = tokenizer(data['text'], max_length=3, padding=True)\n",
    "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35764f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts_truncation = tokenizer(data['text'], max_length=3, truncation=True)\n",
    "print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ee8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(data[\"text\"], max_length=3, truncation=True)\n",
    "print(\"Uing left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16375bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts_both = tokenizer(data[\"text\"], max_length=3, truncation=True, padding=True)\n",
    "print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb036aee",
   "metadata": {},
   "source": [
    "### Generate Question Answer Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the FLAN-T5 generator\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=256, do_sample=True, top_p=0.95)\n",
    "\n",
    "# Your source text\n",
    "raw_text = data[\"text\"][0].strip()\n",
    "\n",
    "# Better Prompt Engineering\n",
    "question_prompt = f\"\"\"Given the following passage, generate a detailed, insightful, and specific question that tests comprehension:\n",
    "\n",
    "Passage:\n",
    "\\\"\\\"\\\"{raw_text}\\\"\\\"\\\"\n",
    "\n",
    "Question:\"\"\"\n",
    "\n",
    "# Generate a question\n",
    "question_output = generator(question_prompt, max_length=100, num_return_sequences=1)[0]\n",
    "question = question_output[\"generated_text\"].strip()\n",
    "\n",
    "# Better Answer Prompt\n",
    "answer_prompt = f\"\"\"Given the following passage and question, provide an accurate and complete answer strictly based on the passage content.\n",
    "\n",
    "Passage:\n",
    "\\\"\\\"\\\"{raw_text}\\\"\\\"\\\"\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Generate an answer\n",
    "answer_output = generator(answer_prompt, max_length=150, num_return_sequences=1)[0]\n",
    "answer = answer_output[\"generated_text\"].strip()\n",
    "\n",
    "# Print results\n",
    "print(\"Generated Question:\")\n",
    "print(question)\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bfc879",
   "metadata": {},
   "source": [
    "### Prepare instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a28f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# filename = \"lamini_docs.jsonl\"\n",
    "filename = data_file_l[0]\n",
    "instruction_dataset_df = pd.read_json(filename, lines=False)\n",
    "examples = instruction_dataset_df.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9fa22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]\n",
    "\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"question\"][i]\n",
    "  answer = examples[\"answer\"][i]\n",
    "  text_with_prompt_template = prompt_template.format(question=question)\n",
    "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"One datapoint in the finetuning dataset:\")\n",
    "pprint(finetuning_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fcec6",
   "metadata": {},
   "source": [
    "### Creating a HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b928fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "data_dir = \"../data/prompt_response/\"\n",
    "data_file_l = glob(data_dir + \"*.json\")\n",
    "data_file_l[0]\n",
    "with open(data_file_l[0], 'rb') as f:\n",
    "    data = json.load(f)\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e52e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset = dataset.train_test_split(test_size=0.05) # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using meta-llama/Meta-Llama-3-8B\n",
    "from transformers import AutoTokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "def tokenize(example):\n",
    "        return tokenizer(example(['text'], truncation=True, padding='max_length', max_length=512))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file_l[0], 'rb') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd003e9",
   "metadata": {},
   "source": [
    "### Full Example Pipeline: \n",
    "Vector Store, QLoRA Weights, HRLF, Custom Style Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607a2d4",
   "metadata": {},
   "source": [
    "              ┌────────────────────┐\n",
    "              │ Base LLM (LLaMA 3) │\n",
    "              └────────┬───────────┘\n",
    "                       │\n",
    "         ┌─────────────▼────────────┐\n",
    "         │  QLoRA Adapter Loader    │ ←─ Avatar (\"Mom\", \"Friend\", etc.)\n",
    "         └─────────────┬────────────┘\n",
    "                       │\n",
    "         ┌─────────────▼─────────────┐\n",
    "         │ LangChain Memory Manager  │\n",
    "         └─────────────┬─────────────┘\n",
    "                       │\n",
    "              ┌────────▼─────────┐\n",
    "              │  Prompt Builder  │ ←─ Style + Episodic + Semantic memory\n",
    "              └────────┬─────────┘\n",
    "                       │\n",
    "              ┌────────▼──────────┐\n",
    "              │     LLM Output    │\n",
    "              └───────────────────┘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff67c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a051693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ground Truth Dataset\n",
    "data_dir = \"../data/prompt_response/\"\n",
    "data_file_l = glob(data_dir + \"*.json\")\n",
    "data_file_l[0]\n",
    "with open(data_file_l[0], 'rb') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Embed your dataset\n",
    "documents = [\n",
    "    {\"page_content\": s, \"metadata\": {\"source\": f\"statement_{i}\"}}\n",
    "    for i, s in enumerate(data['text'])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa0f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0][\"page_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4120a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75900905",
   "metadata": {},
   "source": [
    "### Using QLoRA with LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd063b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "\n",
    "from glob import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47048bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Enable 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # or torch.float32 if you have more memory\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Choose from \"nf4\" or \"fp4\"\n",
    ")\n",
    "\n",
    "# Load tokenizer and quantized model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Example message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "# Tokenize with chat template (for LLaMA-3-style formatting)\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "\n",
    "# Decode and print the generated text\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f44f9",
   "metadata": {},
   "source": [
    "### Instructions for QLoRA Fine-tuning, Vector Stores, & Custom Style Classifiers https://chatgpt.com/share/688ae919-ee40-8011-ab40-2b52a0c3db06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa11d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux-pc/anaconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-31 17:44:05.319328: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-31 17:44:05.433276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753998245.477661  310875 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753998245.490341  310875 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753998245.585475  310875 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753998245.585490  310875 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753998245.585492  310875 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753998245.585493  310875 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-31 17:44:05.596198: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Base model and tokenizer (QLoRA-ready model)\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "ADAPTER = \"meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\"\n",
    "\n",
    "PEFT_DIR = \"./qlora_adapter\"  # Directory to save/load adapter weights\n",
    "VECTORSTORE_DIR = \"./chroma_db\"\n",
    "\n",
    "# Sentence embedding model for vector store and evaluation\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Setup Chroma client and collection\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "client = PersistentClient(path=VECTORSTORE_DIR)\n",
    "collection = client.get_or_create_collection(name=\"style_memory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac54a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing LoRA adapter from ./qlora_adapter/checkpoint-18/\n",
      "trainable params: 0 || all params: 3,215,043,584 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "PEFT_DIR = \"./qlora_adapter/checkpoint-18/\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LOAD_MODEL = True\n",
    "\n",
    "def load_model_and_tokenizer_for_qlora():\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # 4-bit quantization config for QLoRA\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    # Load model in 4-bit for QLoRA\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # If adapter weights already exist, load them\n",
    "    if LOAD_MODEL:\n",
    "        print(f\"Loading existing LoRA adapter from {PEFT_DIR}\")\n",
    "        # Load the fine-tuned QLoRA adapter weights with PeftModel.from_pretrained\n",
    "        # lora_dir = PEFT_DIR.rstrip(\"./\")\n",
    "        # print(f\"{lora_dir}\")\n",
    "        model = PeftModel.from_pretrained(model, PEFT_DIR)\n",
    "    else:\n",
    "        print(\"Preparing Model for Training\")\n",
    "        # Prepare for k-bit training (adds norm casting, disables gradients on frozen parts, etc.)\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "        # LoRA adapter configuration (adapt r, alpha, target_modules as needed)\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "\n",
    "        # Wrap the model with PEFT\n",
    "        model = get_peft_model(model, lora_config)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer_for_qlora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedder.encode(docs).tolist()\n",
    "ids = [f\"doc_{i}\" for i in range(len(docs))]\n",
    "\n",
    "collection.add(documents=docs, embeddings=embeddings, ids=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb429d1",
   "metadata": {},
   "source": [
    "### Training QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "data_dir = \"../data/prompt_response/\"\n",
    "data_file_l = glob(data_dir + \"*.json\")\n",
    "data_file_l[0]\n",
    "with open(data_file_l[0], 'rb') as f:\n",
    "    data = json.load(f)\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # labels are input_ids with padding tokens masked as -100 to ignore in loss\n",
    "    labels = []\n",
    "    for input_ids in examples[\"input_ids\"]:\n",
    "        label = input_ids.copy()\n",
    "        # Mask padding tokens\n",
    "        label = [-100 if token == tokenizer.pad_token_id else token for token in label]\n",
    "        labels.append(label)\n",
    "    examples[\"labels\"] = labels\n",
    "    return examples\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(group_texts, batched=True)\n",
    "\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split['train']\n",
    "eval_dataset = split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7e7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora_adapter\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    bf16=True if torch.cuda.is_bf16_supported() else False,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525fd3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22f1fb",
   "metadata": {},
   "source": [
    "### Testing the qlora adapter manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4607951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer:\n",
      " I love you too\n"
     ]
    }
   ],
   "source": [
    "# Input prompt\n",
    "input_text = \"Q: I love you a lot.\\nA:\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
    "\n",
    "# Make sure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation (important for inference speed and memory)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=5,\n",
    "        do_sample=False,  # Deterministic output\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id  # Stop at EOS\n",
    "    )\n",
    "\n",
    "# Decode generated token IDs into text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract answer portion after 'A:'\n",
    "if \"A:\" in generated_text:\n",
    "    answer = generated_text.split(\"A:\", 1)[1].strip()\n",
    "else:\n",
    "    answer = generated_text.strip()\n",
    "\n",
    "print(\"Generated answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ee817b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "data_dir = \"../data/prompt_response/\"\n",
    "data_file_l = glob(data_dir + \"*.json\")\n",
    "data_file_l[0]\n",
    "with open(data_file_l[0], 'rb') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84ac66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedder.encode(data['text']).tolist()\n",
    "ids = [f\"doc_{i}\" for i in range(len(data['text']))]\n",
    "\n",
    "collection.add(documents=data['text'], embeddings=embeddings, ids=ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f834d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_context(user_input: str, top_k: int = 10, max_new_tokens: int = 50) -> str:\n",
    "    # Embed query and retrieve from vector DB\n",
    "    query_vec = embedder.encode(user_input).tolist()\n",
    "    results = collection.query(query_embeddings=[query_vec], n_results=top_k)\n",
    "    print(f\"results: {results}\")\n",
    "\n",
    "    docs = results.get(\"documents\", [[]])[0]\n",
    "    if not docs:\n",
    "        context = \"No relevant context found.\"\n",
    "    else:\n",
    "        # Optionally truncate context length for model input token limits\n",
    "        # Here we join and limit length (e.g., first 1000 chars)\n",
    "        context = \"\\n\".join(docs)\n",
    "        context = context[:1000]\n",
    "\n",
    "    # Improved prompt with explicit instruction and clear delimiters\n",
    "    prompt = f\"\"\"You are the person in the contextual statements. Use the context to answer the question briefly and only once.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Q: {user_input}\n",
    "A:\"\"\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
    "\n",
    "    # Generate answer\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Set True to enable sampling\n",
    "            # temperature=0.7,  # Uncomment if do_sample=True\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract text after last \"A:\" in case prompt or output has multiple\n",
    "    answer = decoded.split(\"A:\")[-1].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67cdb1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results: {'ids': [['doc_40', 'doc_28', 'doc_18', 'doc_4', 'doc_7', 'doc_8', 'doc_32', 'doc_29', 'doc_19', 'doc_21']], 'embeddings': None, 'documents': [[\"I'm here\", 'I love YOU!!!', 'I love you Evan Woods!', 'Love you both ..... I am fine', 'Me too Evan ... me too', 'Me too Evan ... me too', 'I love you too sweetie.... Thanks for your help!', 'Hey buddy! Are you on your way???? If not, can quickly ask Steve if he can bring me home....', 'Good night', 'My Bubby']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[None, None, None, None, None, None, None, None, None, None]], 'distances': [[1.173533320426941, 1.249318242073059, 1.3540077209472656, 1.376650094985962, 1.3858208656311035, 1.3858208656311035, 1.4112266302108765, 1.4549580812454224, 1.4601541757583618, 1.4601597785949707]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I miss you too sweetie.... I am fine.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_context(user_input=\"I miss you so much\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
