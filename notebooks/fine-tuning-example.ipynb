{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849869a5",
   "metadata": {},
   "source": [
    "# Fine-tuning LLMs on personal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82f4b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c01abc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/prompt_response/\"\n",
    "data_file_l = glob(data_dir + \"*.json\")\n",
    "data_file_l[0]\n",
    "with open(data_file_l[0], 'rb') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccba1b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where r u?',\n",
       " 'Where r u?',\n",
       " \"How much was lowes? I'll reimburse you if I can....\",\n",
       " \"Ms Melissa just passed!!!! I bet she's bitching God out for taking her away from her family, about now!\",\n",
       " 'Love you both ..... I am fine',\n",
       " \"It's ok now ... everything is ok now ... she's no longer struggling and no longer in pain ....\",\n",
       " \"It's ok now ... everything is ok now ... she's no longer struggling and no longer in pain ....\",\n",
       " 'Me too Evan ... me too',\n",
       " 'Me too Evan ... me too',\n",
       " 'Yes .... Travis is a mess right now .... Those things will b determined over the next couple of days ....',\n",
       " \"Of course honey .... I'm helping Travis with some arrangements & I'll let you know when I'm leaving here.... It shouldn't be much longer....\",\n",
       " 'Evan call me plz',\n",
       " 'Thanks for sharing!!! It looks like you to are great pals!!!!',\n",
       " \"I also saw Lindsay little belly!!!! Love it! Lindsay you look beautiful! I just can't wait to see your little girl!!!\",\n",
       " \"If it's going to b this cold, I want snow!!!!\",\n",
       " \"I'm bringing breakfast home!\",\n",
       " \"It's too bad outside for you 2 to go anywhere .... I'm in Mt Pleasant at the Broken Egg Cafe .... Yum!\",\n",
       " 'Ditto!!!!',\n",
       " 'I love you Evan Woods!',\n",
       " 'Good night',\n",
       " '1963....Big sis and baby bro',\n",
       " 'My Bubby',\n",
       " 'Here! Calle when ur done 😊',\n",
       " 'Are you awake????',\n",
       " \"I'm at Berkeley hospital monitoring patients ... I'll b home by 1230 at the latest! Let's do something today!!! 😴\",\n",
       " 'Where would you like to go??? Beaufort? Road trip & eat in the car ... or go to that new restaurant in NChar???',\n",
       " 'Just a little city ... quaint ....',\n",
       " 'Do you think you could take that green & white card out of my wallet & pick up 2-3 prescriptions for me at Walgreens pharmacy when they open? I think they open at 9am',\n",
       " 'I love YOU!!!',\n",
       " 'Hey buddy! Are you on your way???? If not, can quickly ask Steve if he can bring me home....',\n",
       " 'I have to know now ....',\n",
       " \"I'm heading to the car\",\n",
       " 'I love you too sweetie.... Thanks for your help!',\n",
       " 'So far so good!',\n",
       " 'Thanks for bringing me to work tonight! Sweet dreams handsome!!! Love u bunches!!!!',\n",
       " \"Ok .... I'll b a couple minutes yet .... Sorry\",\n",
       " 'On my way',\n",
       " \"No ... let's plan on asking off!!!\",\n",
       " \"Well I'm feeling ok so far ... slight headache but if you want to drive out of town we can .... Let's just see how I'm feeling when I get home! I'm here til noon today!\",\n",
       " \"I over slept ... they didn't need me ... I went shopping .... B home soon .... Thought I would sauté chicken & make stir fry veggies .... U interested?\",\n",
       " \"I'm here\",\n",
       " 'Thank you Evan!!!',\n",
       " 'Thank you Evan!!!',\n",
       " 'What???? How????',\n",
       " \"I'm going to stop at roper express car before I come home.... Get checked out\",\n",
       " 'You are a good person']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b11e05",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## [Tutorial Link ](https://learn.deeplearning.ai/courses/finetuning-large-language-models/lesson/vl60i/training-process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a450e",
   "metadata": {},
   "source": [
    "### Data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dab40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f56656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed83a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5442bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(text)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84687411",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f524d3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, how are you?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19be4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize multiple texts at once\n",
    "encoded_texts = tokenizer(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f8a55f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded several texts:  {'input_ids': [[7161, 391, 1484, 32], [7161, 391, 1484, 32], [2347, 1199, 369, 1698, 265, 32, 309, 1833, 27533, 368, 604, 309, 476, 2391], [12822, 38994, 816, 4817, 18963, 309, 701, 703, 434, 2372, 7695, 2656, 562, 323, 3192, 617, 1977, 432, 617, 2021, 13, 670, 1024, 2], [23337, 368, 1097, 10712, 1051, 309, 717, 4030], [1147, 434, 8718, 1024, 3346, 3253, 310, 8718, 1024, 3346, 703, 434, 642, 3356, 15586, 285, 642, 3356, 275, 3075, 22833], [1147, 434, 8718, 1024, 3346, 3253, 310, 8718, 1024, 3346, 703, 434, 642, 3356, 15586, 285, 642, 3356, 275, 3075, 22833], [5072, 1512, 37144, 3346, 479, 1512], [5072, 1512, 37144, 3346, 479, 1512], [4374, 22833, 35382, 310, 247, 4840, 987, 1024, 22833, 9240, 1841, 588, 270, 3413, 689, 253, 1735, 4564, 273, 1897, 22833], [4527, 2282, 14795, 22833, 309, 1353, 9073, 35382, 342, 690, 16669, 708, 309, 1833, 1339, 368, 871, 672, 309, 1353, 6108, 1060, 2391, 733, 10095, 626, 320, 1199, 3356, 2391], [38, 6148, 1067, 479, 499, 91], [8061, 323, 9628, 15844, 733, 4453, 751, 368, 281, 403, 1270, 39839, 18963], [42, 671, 3047, 44056, 1652, 23828, 18963, 10540, 352, 2, 44056, 368, 1007, 5389, 2, 309, 816, 476, 626, 3343, 281, 923, 634, 1652, 3226, 15844], [2042, 352, 434, 1469, 281, 270, 436, 5412, 13, 309, 971, 8762, 18963], [42, 1353, 9745, 12561, 1728, 2], [1147, 434, 1512, 3076, 3345, 323, 368, 374, 281, 564, 9825, 22833, 309, 1353, 275, 30480, 21231, 10597, 387, 253, 4819, 3612, 30112, 45013, 22833, 714, 360, 2], [37, 35570, 18963], [42, 2389, 368, 37144, 26496, 2], [8620, 2360], [30488, 2391, 5178, 49144, 285, 6858, 1795], [3220, 378, 538, 1615], [4943, 2, 2263, 282, 672, 2936, 2218, 49042, 221], [6723, 368, 22238, 25561], [42, 1353, 387, 24538, 4675, 8667, 1363, 3346, 309, 1833, 270, 1728, 407, 1249, 1229, 387, 253, 6323, 2, 1281, 434, 513, 1633, 3063, 15844, 49042, 114], [7161, 651, 368, 751, 281, 564, 33186, 42224, 4371, 32, 8669, 7408, 708, 6008, 275, 253, 1113, 3346, 390, 564, 281, 326, 747, 10301, 275, 427, 8662, 33186], [6300, 247, 1652, 2846, 3346, 572, 1143, 22833], [4045, 368, 1158, 368, 812, 1379, 326, 4759, 708, 3168, 3120, 562, 273, 619, 23138, 708, 2619, 598, 374, 14, 20, 37971, 323, 479, 387, 8178, 72, 10436, 33173, 672, 597, 1527, 32, 309, 1158, 597, 1527, 387, 898, 312], [42, 2389, 8702, 15844], [8262, 29517, 2, 6272, 368, 327, 634, 1039, 25561, 1310, 417, 13, 476, 4541, 1642, 11819, 604, 344, 476, 3324, 479, 1728, 2391], [42, 452, 281, 871, 1024, 22833], [42, 1353, 13590, 281, 253, 1113], [42, 2389, 368, 1512, 7353, 466, 2391, 9375, 323, 634, 1361, 2], [2598, 2080, 594, 1175, 2], [8061, 323, 9745, 479, 281, 789, 11608, 2, 26048, 13702, 23639, 15844, 10540, 1484, 34258, 2706, 18963], [20127, 22833, 309, 1833, 270, 247, 4564, 2909, 2568, 22833, 26070], [2374, 619, 1039], [2302, 3346, 1339, 434, 2098, 327, 7004, 745, 15844], [4497, 309, 1353, 5471, 8718, 594, 2080, 3346, 4512, 24934, 533, 604, 368, 971, 281, 4446, 562, 273, 3874, 359, 476, 22833, 1281, 434, 816, 923, 849, 309, 1353, 5471, 672, 309, 755, 1728, 2, 309, 1353, 1060, 10751, 23483, 3063, 2], [42, 689, 21567, 3346, 597, 1904, 626, 878, 479, 3346, 309, 2427, 12701, 22833, 378, 1728, 3517, 22833, 29084, 309, 651, 618, 307, 860, 11152, 708, 1056, 10576, 39622, 43697, 447, 22833, 530, 6110, 32], [42, 1353, 1060], [8398, 368, 37144, 15844], [8398, 368, 37144, 15844], [1276, 25561, 1359, 25561], [42, 1353, 1469, 281, 3523, 387, 687, 468, 3890, 1113, 1078, 309, 1705, 1728, 2391, 5057, 10141, 562], [1394, 403, 247, 1175, 1436]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded several texts: \", encoded_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71373fc6",
   "metadata": {},
   "source": [
    "### Padding and truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f063858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using padding:  [[7161, 391, 1484, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7161, 391, 1484, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2347, 1199, 369, 1698, 265, 32, 309, 1833, 27533, 368, 604, 309, 476, 2391, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [12822, 38994, 816, 4817, 18963, 309, 701, 703, 434, 2372, 7695, 2656, 562, 323, 3192, 617, 1977, 432, 617, 2021, 13, 670, 1024, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [23337, 368, 1097, 10712, 1051, 309, 717, 4030, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1147, 434, 8718, 1024, 3346, 3253, 310, 8718, 1024, 3346, 703, 434, 642, 3356, 15586, 285, 642, 3356, 275, 3075, 22833, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1147, 434, 8718, 1024, 3346, 3253, 310, 8718, 1024, 3346, 703, 434, 642, 3356, 15586, 285, 642, 3356, 275, 3075, 22833, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5072, 1512, 37144, 3346, 479, 1512, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5072, 1512, 37144, 3346, 479, 1512, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4374, 22833, 35382, 310, 247, 4840, 987, 1024, 22833, 9240, 1841, 588, 270, 3413, 689, 253, 1735, 4564, 273, 1897, 22833, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4527, 2282, 14795, 22833, 309, 1353, 9073, 35382, 342, 690, 16669, 708, 309, 1833, 1339, 368, 871, 672, 309, 1353, 6108, 1060, 2391, 733, 10095, 626, 320, 1199, 3356, 2391, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [38, 6148, 1067, 479, 499, 91, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8061, 323, 9628, 15844, 733, 4453, 751, 368, 281, 403, 1270, 39839, 18963, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [42, 671, 3047, 44056, 1652, 23828, 18963, 10540, 352, 2, 44056, 368, 1007, 5389, 2, 309, 816, 476, 626, 3343, 281, 923, 634, 1652, 3226, 15844, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2042, 352, 434, 1469, 281, 270, 436, 5412, 13, 309, 971, 8762, 18963, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [42, 1353, 9745, 12561, 1728, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1147, 434, 1512, 3076, 3345, 323, 368, 374, 281, 564, 9825, 22833, 309, 1353, 275, 30480, 21231, 10597, 387, 253, 4819, 3612, 30112, 45013, 22833, 714, 360, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [37, 35570, 18963, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [42, 2389, 368, 37144, 26496, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8620, 2360, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [30488, 2391, 5178, 49144, 285, 6858, 1795, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3220, 378, 538, 1615, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4943, 2, 2263, 282, 672, 2936, 2218, 49042, 221, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6723, 368, 22238, 25561, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [42, 1353, 387, 24538, 4675, 8667, 1363, 3346, 309, 1833, 270, 1728, 407, 1249, 1229, 387, 253, 6323, 2, 1281, 434, 513, 1633, 3063, 15844, 49042, 114, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7161, 651, 368, 751, 281, 564, 33186, 42224, 4371, 32, 8669, 7408, 708, 6008, 275, 253, 1113, 3346, 390, 564, 281, 326, 747, 10301, 275, 427, 8662, 33186, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6300, 247, 1652, 2846, 3346, 572, 1143, 22833, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4045, 368, 1158, 368, 812, 1379, 326, 4759, 708, 3168, 3120, 562, 273, 619, 23138, 708, 2619, 598, 374, 14, 20, 37971, 323, 479, 387, 8178, 72, 10436, 33173, 672, 597, 1527, 32, 309, 1158, 597, 1527, 387, 898, 312, 0, 0], [42, 2389, 8702, 15844, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8262, 29517, 2, 6272, 368, 327, 634, 1039, 25561, 1310, 417, 13, 476, 4541, 1642, 11819, 604, 344, 476, 3324, 479, 1728, 2391, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [42, 452, 281, 871, 1024, 22833, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [42, 1353, 13590, 281, 253, 1113, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [42, 2389, 368, 1512, 7353, 466, 2391, 9375, 323, 634, 1361, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2598, 2080, 594, 1175, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8061, 323, 9745, 479, 281, 789, 11608, 2, 26048, 13702, 23639, 15844, 10540, 1484, 34258, 2706, 18963, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [20127, 22833, 309, 1833, 270, 247, 4564, 2909, 2568, 22833, 26070, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2374, 619, 1039, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2302, 3346, 1339, 434, 2098, 327, 7004, 745, 15844, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4497, 309, 1353, 5471, 8718, 594, 2080, 3346, 4512, 24934, 533, 604, 368, 971, 281, 4446, 562, 273, 3874, 359, 476, 22833, 1281, 434, 816, 923, 849, 309, 1353, 5471, 672, 309, 755, 1728, 2, 309, 1353, 1060, 10751, 23483, 3063, 2], [42, 689, 21567, 3346, 597, 1904, 626, 878, 479, 3346, 309, 2427, 12701, 22833, 378, 1728, 3517, 22833, 29084, 309, 651, 618, 307, 860, 11152, 708, 1056, 10576, 39622, 43697, 447, 22833, 530, 6110, 32, 0, 0, 0, 0, 0, 0, 0], [42, 1353, 1060, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8398, 368, 37144, 15844, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8398, 368, 37144, 15844, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1276, 25561, 1359, 25561, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [42, 1353, 1469, 281, 3523, 387, 687, 468, 3890, 1113, 1078, 309, 1705, 1728, 2391, 5057, 10141, 562, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1394, 403, 247, 1175, 1436, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux-pc/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2663: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encoded_texts_longest = tokenizer(data['text'], max_length=3, padding=True)\n",
    "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35764f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using truncation:  [[7161, 391, 1484], [7161, 391, 1484], [2347, 1199, 369], [12822, 38994, 816], [23337, 368, 1097], [1147, 434, 8718], [1147, 434, 8718], [5072, 1512, 37144], [5072, 1512, 37144], [4374, 22833, 35382], [4527, 2282, 14795], [38, 6148, 1067], [8061, 323, 9628], [42, 671, 3047], [2042, 352, 434], [42, 1353, 9745], [1147, 434, 1512], [37, 35570, 18963], [42, 2389, 368], [8620, 2360], [30488, 2391, 5178], [3220, 378, 538], [4943, 2, 2263], [6723, 368, 22238], [42, 1353, 387], [7161, 651, 368], [6300, 247, 1652], [4045, 368, 1158], [42, 2389, 8702], [8262, 29517, 2], [42, 452, 281], [42, 1353, 13590], [42, 2389, 368], [2598, 2080, 594], [8061, 323, 9745], [20127, 22833, 309], [2374, 619, 1039], [2302, 3346, 1339], [4497, 309, 1353], [42, 689, 21567], [42, 1353, 1060], [8398, 368, 37144], [8398, 368, 37144], [1276, 25561, 1359], [42, 1353, 1469], [1394, 403, 247]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_truncation = tokenizer(data['text'], max_length=3, truncation=True)\n",
    "print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb7ee8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uing left-side truncation:  [[391, 1484, 32], [391, 1484, 32], [309, 476, 2391], [670, 1024, 2], [309, 717, 4030], [275, 3075, 22833], [275, 3075, 22833], [3346, 479, 1512], [3346, 479, 1512], [273, 1897, 22833], [1199, 3356, 2391], [479, 499, 91], [1270, 39839, 18963], [1652, 3226, 15844], [971, 8762, 18963], [12561, 1728, 2], [714, 360, 2], [37, 35570, 18963], [37144, 26496, 2], [8620, 2360], [285, 6858, 1795], [378, 538, 1615], [2218, 49042, 221], [368, 22238, 25561], [15844, 49042, 114], [427, 8662, 33186], [572, 1143, 22833], [387, 898, 312], [2389, 8702, 15844], [479, 1728, 2391], [871, 1024, 22833], [281, 253, 1113], [634, 1361, 2], [594, 1175, 2], [34258, 2706, 18963], [2568, 22833, 26070], [2374, 619, 1039], [7004, 745, 15844], [23483, 3063, 2], [530, 6110, 32], [42, 1353, 1060], [368, 37144, 15844], [368, 37144, 15844], [25561, 1359, 25561], [5057, 10141, 562], [247, 1175, 1436]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(data[\"text\"], max_length=3, truncation=True)\n",
    "print(\"Uing left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16375bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using both padding and truncation:  [[391, 1484, 32], [391, 1484, 32], [309, 476, 2391], [670, 1024, 2], [309, 717, 4030], [275, 3075, 22833], [275, 3075, 22833], [3346, 479, 1512], [3346, 479, 1512], [273, 1897, 22833], [1199, 3356, 2391], [479, 499, 91], [1270, 39839, 18963], [1652, 3226, 15844], [971, 8762, 18963], [12561, 1728, 2], [714, 360, 2], [37, 35570, 18963], [37144, 26496, 2], [8620, 2360, 0], [285, 6858, 1795], [378, 538, 1615], [2218, 49042, 221], [368, 22238, 25561], [15844, 49042, 114], [427, 8662, 33186], [572, 1143, 22833], [387, 898, 312], [2389, 8702, 15844], [479, 1728, 2391], [871, 1024, 22833], [281, 253, 1113], [634, 1361, 2], [594, 1175, 2], [34258, 2706, 18963], [2568, 22833, 26070], [2374, 619, 1039], [7004, 745, 15844], [23483, 3063, 2], [530, 6110, 32], [42, 1353, 1060], [368, 37144, 15844], [368, 37144, 15844], [25561, 1359, 25561], [5057, 10141, 562], [247, 1175, 1436]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_both = tokenizer(data[\"text\"], max_length=3, truncation=True, padding=True)\n",
    "print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb036aee",
   "metadata": {},
   "source": [
    "### Generate Question Answer Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbd3f979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux-pc/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Question:\n",
      "Which is the title of the passage?\n",
      "\n",
      "Generated Answer:\n",
      "Where r U?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the FLAN-T5 generator\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=256, do_sample=True, top_p=0.95)\n",
    "\n",
    "# Your source text\n",
    "raw_text = data[\"text\"][0].strip()\n",
    "\n",
    "# Better Prompt Engineering\n",
    "question_prompt = f\"\"\"Given the following passage, generate a detailed, insightful, and specific question that tests comprehension:\n",
    "\n",
    "Passage:\n",
    "\\\"\\\"\\\"{raw_text}\\\"\\\"\\\"\n",
    "\n",
    "Question:\"\"\"\n",
    "\n",
    "# Generate a question\n",
    "question_output = generator(question_prompt, max_length=100, num_return_sequences=1)[0]\n",
    "question = question_output[\"generated_text\"].strip()\n",
    "\n",
    "# Better Answer Prompt\n",
    "answer_prompt = f\"\"\"Given the following passage and question, provide an accurate and complete answer strictly based on the passage content.\n",
    "\n",
    "Passage:\n",
    "\\\"\\\"\\\"{raw_text}\\\"\\\"\\\"\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Generate an answer\n",
    "answer_output = generator(answer_prompt, max_length=150, num_return_sequences=1)[0]\n",
    "answer = answer_output[\"generated_text\"].strip()\n",
    "\n",
    "# Print results\n",
    "print(\"Generated Question:\")\n",
    "print(question)\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4932d15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a good person'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bfc879",
   "metadata": {},
   "source": [
    "### Prepare instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "262a28f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# filename = \"lamini_docs.jsonl\"\n",
    "filename = data_file_l[0]\n",
    "instruction_dataset_df = pd.read_json(filename, lines=False)\n",
    "examples = instruction_dataset_df.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9fa22c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (706917710.py, line 10)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprompt_template = \"\"\"### Question:\u001b[39m\n                      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]\n",
    "\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"question\"][i]\n",
    "  answer = examples[\"answer\"][i]\n",
    "  text_with_prompt_template = prompt_template.format(question=question)\n",
    "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"One datapoint in the finetuning dataset:\")\n",
    "pprint(finetuning_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fcec6",
   "metadata": {},
   "source": [
    "### Creating a HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b928fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "data_dir = \"../data/prompt_response/\"\n",
    "data_file_l = glob(data_dir + \"*.json\")\n",
    "data_file_l[0]\n",
    "with open(data_file_l[0], 'rb') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77e52e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(data)\n",
    "# dataset = dataset.train_test_split(test_size=0.05) # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux-pc/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n403 Client Error. (Request ID: Root=1-688ac855-1afcbdb465e81f182a474df3;2118dbad-8d8a-4c7a-9438-f4d3d38c2465)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-8B to ask for access.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGatedRepoError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/transformers/utils/hub.py:398\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     resolved_file = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/file_download.py:1117\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1116\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/file_download.py:1658\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1654\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1655\u001b[39m ):\n\u001b[32m   1656\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1658\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1659\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1660\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/file_download.py:1546\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1546\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/file_download.py:1463\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1462\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1471\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/file_download.py:310\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    309\u001b[39m response = http_backoff(method=method, url=url, **params, retry_on_exceptions=(), retry_on_status_codes=(\u001b[32m429\u001b[39m,))\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:426\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    423\u001b[39m     message = (\n\u001b[32m    424\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    425\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_message == \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mGatedRepoError\u001b[39m: 403 Client Error. (Request ID: Root=1-688ac855-1afcbdb465e81f182a474df3;2118dbad-8d8a-4c7a-9438-f4d3d38c2465)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-8B to ask for access.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m      3\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Meta-Llama-3-8B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize\u001b[39m(example):\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(example([\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m], truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, padding=\u001b[33m'\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m'\u001b[39m, max_length=\u001b[32m512\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:782\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    781\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m         config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    785\u001b[39m     config_tokenizer_class = config.tokenizer_class\n\u001b[32m    786\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoTokenizer\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1111\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1109\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1111\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1113\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/transformers/configuration_utils.py:633\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    631\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[32m    635\u001b[39m     original_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = config_dict[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/transformers/configuration_utils.py:688\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    684\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME)\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    687\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m     commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[32m    704\u001b[39m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[32m    705\u001b[39m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/transformers/utils/hub.py:416\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    417\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure to have access to it at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    419\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    422\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    423\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlisted on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    424\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    425\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`token=<your_token>`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    426\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n403 Client Error. (Request ID: Root=1-688ac855-1afcbdb465e81f182a474df3;2118dbad-8d8a-4c7a-9438-f4d3d38c2465)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-8B to ask for access."
     ]
    }
   ],
   "source": [
    "# Using meta-llama/Meta-Llama-3-8B\n",
    "from transformers import AutoTokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "def tokenize(example):\n",
    "        return tokenizer(example(['text'], truncation=True, padding='max_length', max_length=512))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file_l[0], 'rb') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd003e9",
   "metadata": {},
   "source": [
    "### Full Example Pipeline: \n",
    "Vector Store, QLoRA Weights, HRLF, Custom Style Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607a2d4",
   "metadata": {},
   "source": [
    "              ┌────────────────────┐\n",
    "              │ Base LLM (LLaMA 3) │\n",
    "              └────────┬───────────┘\n",
    "                       │\n",
    "         ┌─────────────▼────────────┐\n",
    "         │  QLoRA Adapter Loader    │ ←─ Avatar (\"Mom\", \"Friend\", etc.)\n",
    "         └─────────────┬────────────┘\n",
    "                       │\n",
    "         ┌─────────────▼─────────────┐\n",
    "         │ LangChain Memory Manager  │\n",
    "         └─────────────┬─────────────┘\n",
    "                       │\n",
    "              ┌────────▼─────────┐\n",
    "              │  Prompt Builder  │ ←─ Style + Episodic + Semantic memory\n",
    "              └────────┬─────────┘\n",
    "                       │\n",
    "              ┌────────▼──────────┐\n",
    "              │     LLM Output    │\n",
    "              └───────────────────┘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff67c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a051693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ground Truth Dataset\n",
    "data_dir = \"../data/prompt_response/\"\n",
    "data_file_l = glob(data_dir + \"*.json\")\n",
    "data_file_l[0]\n",
    "with open(data_file_l[0], 'rb') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd7b1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Embed your dataset\n",
    "documents = [\n",
    "    {\"page_content\": s, \"metadata\": {\"source\": f\"statement_{i}\"}}\n",
    "    for i, s in enumerate(data['text'])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2afa0f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Where r u?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0][\"page_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4120a801",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create vector store\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m vectorstore = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./chroma_db\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:876\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_documents\u001b[39m(\n\u001b[32m    847\u001b[39m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[32m   (...)\u001b[39m\u001b[32m    856\u001b[39m     **kwargs: Any,\n\u001b[32m    857\u001b[39m ) -> Chroma:\n\u001b[32m    858\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a Chroma vectorstore from a list of documents.\u001b[39;00m\n\u001b[32m    859\u001b[39m \n\u001b[32m    860\u001b[39m \u001b[33;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    874\u001b[39m \u001b[33;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[32m    875\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m     texts = \u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    877\u001b[39m     metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    878\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_texts(\n\u001b[32m    879\u001b[39m         texts=texts,\n\u001b[32m    880\u001b[39m         embedding=embedding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    888\u001b[39m         **kwargs,\n\u001b[32m    889\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:876\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_documents\u001b[39m(\n\u001b[32m    847\u001b[39m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[32m   (...)\u001b[39m\u001b[32m    856\u001b[39m     **kwargs: Any,\n\u001b[32m    857\u001b[39m ) -> Chroma:\n\u001b[32m    858\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a Chroma vectorstore from a list of documents.\u001b[39;00m\n\u001b[32m    859\u001b[39m \n\u001b[32m    860\u001b[39m \u001b[33;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    874\u001b[39m \u001b[33;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[32m    875\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m     texts = [\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    877\u001b[39m     metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    878\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_texts(\n\u001b[32m    879\u001b[39m         texts=texts,\n\u001b[32m    880\u001b[39m         embedding=embedding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    888\u001b[39m         **kwargs,\n\u001b[32m    889\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75900905",
   "metadata": {},
   "source": [
    "### Using QLoRA with LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd063b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux-pc/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8 does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_peft_model, LoraConfig, TaskType\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model =  \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:561\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    560\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    565\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    566\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    567\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/custom-voice-bot/lib/python3.11/site-packages/transformers/modeling_utils.py:3234\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3228\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m   3229\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have a file named\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3230\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but there is a file without the variant\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3231\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Use `variant=None` to load this model from those weights.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3232\u001b[39m             )\n\u001b[32m   3233\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3234\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m   3235\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have a file named\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3236\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3237\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3238\u001b[39m             )\n\u001b[32m   3239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[32m   3240\u001b[39m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted\u001b[39;00m\n\u001b[32m   3241\u001b[39m     \u001b[38;5;66;03m# to the original exception.\u001b[39;00m\n\u001b[32m   3242\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8 does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "\n",
    "model =  AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f44f9",
   "metadata": {},
   "source": [
    "### Instructions for QLoRA Fine-tuning, Vector Stores, & Custom Style Classifiers https://chatgpt.com/share/688ae919-ee40-8011-ab40-2b52a0c3db06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d42e50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m.from_pretrained(model_id)\n",
      "\u001b[31mNameError\u001b[39m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom-voice-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
